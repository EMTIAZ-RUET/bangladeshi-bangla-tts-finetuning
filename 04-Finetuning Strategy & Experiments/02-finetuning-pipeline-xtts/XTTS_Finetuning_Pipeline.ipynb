{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XTTS Fine-tuning Pipeline for Bangladeshi Bangla TTS\n",
    "\n",
    "This notebook provides a complete pipeline for fine-tuning XTTS (Cross-lingual Text-To-Speech) model for Bangladeshi Bangla language.\n",
    "\n",
    "## Requirements:\n",
    "- NVIDIA GPU with sufficient VRAM (A100 recommended)\n",
    "- Bangla speech dataset with metadata\n",
    "- Reference audio files for speaker conditioning\n",
    "\n",
    "## Pipeline Overview:\n",
    "1. **Setup Environment** - Clone repository and install dependencies\n",
    "2. **Download Checkpoints** - Get pretrained XTTS model\n",
    "3. **Extend Vocabulary** - Add Bangla characters to vocabulary\n",
    "4. **Train DVAE** - Train the variational autoencoder component\n",
    "5. **Fix Compatibility** - Update torch.load for newer PyTorch versions\n",
    "6. **Train GPT** - Train the GPT component for text conditioning\n",
    "7. **Load Fine-tuned Model** - Load the trained model for inference\n",
    "8. **Generate Speech** - Synthesize Bangla speech with fine-tuned model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment\n",
    "\n",
    "Clone the XTTS fine-tuning repository and install required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the XTTS fine-tuning repository\n",
    "!git clone https://github.com/nguyenhoanganh2002/XTTSv2-Finetuning-for-New-Languages.git\n",
    "%cd XTTSv2-Finetuning-for-New-Languages\n",
    "\n",
    "# Install required packages\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the contents of the repository\n",
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download Pretrained Checkpoints\n",
    "\n",
    "Download the pretrained XTTS model checkpoints that will be used as the starting point for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pretrained XTTS checkpoints\n",
    "!python download_checkpoint.py --output_path checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify checkpoint download\n",
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extend Vocabulary for Bangla\n",
    "\n",
    "Extend the model's vocabulary to include Bangla characters and phonemes from the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extend vocabulary configuration for Bangla language\n",
    "!python extend_vocab_config.py \\\n",
    "    --output_path=checkpoints/ \\\n",
    "    --metadata_path /kaggle/input/dataset-custom/dataset_custom/metadata_train.csv \\\n",
    "    --language bn \\\n",
    "    --extended_vocab_size 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train DVAE (Variational Autoencoder)\n",
    "\n",
    "Train the DVAE component of XTTS which handles audio encoding and decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DVAE component\n",
    "!CUDA_VISIBLE_DEVICES=0,1 python train_dvae_xtts.py \\\n",
    "    --output_path=checkpoints/ \\\n",
    "    --train_csv_path=/kaggle/input/dataset-custom/dataset_custom/metadata_train.csv \\\n",
    "    --eval_csv_path=/kaggle/input/dataset-custom/dataset_custom/metadata_eval.csv \\\n",
    "    --language=\"bn\" \\\n",
    "    --num_epochs=15 \\\n",
    "    --batch_size=64 \\\n",
    "    --lr=1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fix PyTorch Compatibility\n",
    "\n",
    "Update the torch.load function to include `weights_only=False` parameter for compatibility with newer PyTorch versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix torch.load compatibility issue\n",
    "file_path = \"/kaggle/working/XTTSv2-Finetuning-for-New-Languages/TTS/utils/io.py\"\n",
    "\n",
    "# Read the current contents\n",
    "with open(file_path, 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Modify the torch.load line to include weights_only=False\n",
    "new_content = content.replace(\n",
    "    'return torch.load(f, map_location=map_location, **kwargs)',\n",
    "    'return torch.load(f, map_location=map_location, weights_only=False, **kwargs)'\n",
    ")\n",
    "\n",
    "# Write the modified content back\n",
    "with open(file_path, 'w') as file:\n",
    "    file.write(new_content)\n",
    "\n",
    "print(\"‚úÖ PyTorch compatibility fix applied successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the changes (optional - shows the modified file content)\n",
    "with open(file_path, 'r') as file:\n",
    "    modified_content = file.read()\n",
    "    \n",
    "# Show only the relevant part\n",
    "lines = modified_content.split('\\n')\n",
    "for i, line in enumerate(lines):\n",
    "    if 'torch.load' in line:\n",
    "        print(f\"Line {i+1}: {line.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train GPT Component\n",
    "\n",
    "Train the GPT component of XTTS which handles text conditioning and cross-lingual capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GPT component\n",
    "!CUDA_VISIBLE_DEVICES=0 python train_gpt_xtts.py \\\n",
    "    --output_path checkpoints/ \\\n",
    "    --metadatas /kaggle/input/dataset-custom/dataset_custom/metadata_train.csv,/kaggle/input/dataset-custom/dataset_custom/metadata_eval.csv,bn \\\n",
    "    --num_epochs 55 \\\n",
    "    --batch_size 4 \\\n",
    "    --grad_acumm 4 \\\n",
    "    --max_text_length 400 \\\n",
    "    --max_audio_length 330750 \\\n",
    "    --weight_decay 1e-2 \\\n",
    "    --lr 1e-6 \\\n",
    "    --save_step 5000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load Fine-tuned XTTS Model\n",
    "\n",
    "Load the fine-tuned XTTS model for inference and speech synthesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from tqdm import tqdm\n",
    "from underthesea import sent_tokenize\n",
    "\n",
    "from TTS.tts.configs.xtts_config import XttsConfig\n",
    "from TTS.tts.models.xtts import Xtts\n",
    "\n",
    "# Device configuration\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"üñ•Ô∏è  Using device: {device}\")\n",
    "\n",
    "# Model paths (update these paths according to your setup)\n",
    "xtts_checkpoint = \"/kaggle/input/pretrain-weights/3-dataset/best_model.pth\"\n",
    "xtts_config = \"/kaggle/input/pretrain-weights/3-dataset/config.json\"\n",
    "xtts_vocab = \"/kaggle/input/pretrain-weights/3-dataset/vocab.json\"\n",
    "\n",
    "print(f\"üìã Loading config from: {xtts_config}\")\n",
    "print(f\"ü§ñ Loading model from: {xtts_checkpoint}\")\n",
    "print(f\"üìö Loading vocab from: {xtts_vocab}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model configuration and initialize XTTS\n",
    "config = XttsConfig()\n",
    "config.load_json(xtts_config)\n",
    "\n",
    "# Initialize XTTS model\n",
    "XTTS_MODEL = Xtts.init_from_config(config)\n",
    "\n",
    "# Load checkpoint\n",
    "XTTS_MODEL.load_checkpoint(\n",
    "    config, \n",
    "    checkpoint_path=xtts_checkpoint, \n",
    "    vocab_path=xtts_vocab, \n",
    "    use_deepspeed=False\n",
    ")\n",
    "\n",
    "# Move model to device\n",
    "XTTS_MODEL.to(device)\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Prepare Speaker Conditioning\n",
    "\n",
    "Load reference audio and extract speaker embeddings for voice conditioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speaker conditioning setup\n",
    "speaker_audio_file = \"/kaggle/input/dataset-custom/ref1.wav\"\n",
    "lang = \"bn\"  # Bangla language code\n",
    "\n",
    "print(f\"üé§ Loading speaker reference from: {speaker_audio_file}\")\n",
    "\n",
    "# Extract conditioning latents from reference audio\n",
    "gpt_cond_latent, speaker_embedding = XTTS_MODEL.get_conditioning_latents(\n",
    "    audio_path=speaker_audio_file,\n",
    "    gpt_cond_len=XTTS_MODEL.config.gpt_cond_len,\n",
    "    max_ref_length=XTTS_MODEL.config.max_ref_len,\n",
    "    sound_norm_refs=XTTS_MODEL.config.sound_norm_refs,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Speaker conditioning prepared successfully!\")\n",
    "print(f\"üìä GPT conditioning shape: {gpt_cond_latent.shape}\")\n",
    "print(f\"üìä Speaker embedding shape: {speaker_embedding.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Text-to-Speech Synthesis\n",
    "\n",
    "Generate speech from Bangla text using the fine-tuned XTTS model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test text 1: Professional context\n",
    "tts_text = \"‡¶≤‡¶ø‡¶≠‡ßá‡¶∞ ‡¶Ü‡¶¨‡ßá‡¶¶‡¶® ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶∏‡¶Æ‡¶Ø‡¶º Brain station ‡¶è‡¶∞ ‡¶ï‡¶∞‡ßç‡¶Æ‡ßÄ‡¶¶‡ßá‡¶∞‡¶ï‡ßá ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶õ‡ßÅ‡¶ü‡¶ø‡¶∞ ‡¶ï‡¶æ‡¶∞‡¶£ ‡¶∏‡ßç‡¶™‡¶∑‡ßç‡¶ü‡¶≠‡¶æ‡¶¨‡ßá ‡¶¨‡¶≤‡¶§‡ßá ‡¶π‡¶¨‡ßá\"\n",
    "\n",
    "print(f\"üéØ Synthesizing text: {tts_text}\")\n",
    "print(f\"üìù Text length: {len(tts_text)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test text 2: Weather report\n",
    "tts_text = \"‡¶¢‡¶æ‡¶ï‡¶æ‡¶Ø‡¶º ‡¶è‡¶ï‡ßÅ‡¶∂ ‡¶°‡¶ø‡¶ó‡ßç‡¶∞‡¶ø ‡¶∏‡ßá‡¶≤‡¶∏‡¶ø‡¶Ø‡¶º‡¶æ‡¶∏ ‡¶§‡¶æ‡¶™‡¶Æ‡¶æ‡¶§‡ßç‡¶∞‡¶æ‡¶Ø‡¶º ‡¶¨‡¶æ‡¶§‡¶æ‡¶∏‡ßá‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá ‡¶¨‡ßÉ‡¶∑‡ßç‡¶ü‡¶ø‡¶™‡¶æ‡¶§ ‡¶Ü‡¶õ‡ßá\"\n",
    "\n",
    "print(f\"üéØ Synthesizing text: {tts_text}\")\n",
    "print(f\"üìù Text length: {len(tts_text)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text into sentences for better synthesis\n",
    "tts_texts = sent_tokenize(tts_text)\n",
    "print(f\"üìÑ Split into {len(tts_texts)} sentences:\")\n",
    "for i, sentence in enumerate(tts_texts, 1):\n",
    "    print(f\"  {i}. {sentence}\")\n",
    "\n",
    "# Generate speech for each sentence\n",
    "wav_chunks = []\n",
    "print(\"\\nüéµ Generating speech...\")\n",
    "\n",
    "for i, text in enumerate(tqdm(tts_texts, desc=\"Synthesizing\")):\n",
    "    wav_chunk = XTTS_MODEL.inference(\n",
    "        text=text,\n",
    "        language=lang,\n",
    "        gpt_cond_latent=gpt_cond_latent,\n",
    "        speaker_embedding=speaker_embedding,\n",
    "        temperature=0.1,          # More natural but still stable\n",
    "        length_penalty=1.0,\n",
    "        repetition_penalty=10.0,   # Enough to stop loops, not distort speech\n",
    "        top_k=20,                 # More phoneme variety\n",
    "        top_p=0.9,                # Allow smoother probability distribution\n",
    "    )\n",
    "    wav_chunks.append(torch.tensor(wav_chunk[\"wav\"]))\n",
    "    print(f\"  ‚úÖ Sentence {i+1} synthesized ({len(wav_chunk['wav'])} samples)\")\n",
    "\n",
    "# Concatenate all audio chunks\n",
    "out_wav = torch.cat(wav_chunks, dim=0).unsqueeze(0).cpu()\n",
    "print(f\"\\nüéµ Final audio shape: {out_wav.shape}\")\n",
    "print(f\"‚è±Ô∏è  Duration: {out_wav.shape[1] / 24000:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Generated Audio\n",
    "\n",
    "Save the synthesized speech to an audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "# Ensure tensor format\n",
    "if not isinstance(out_wav, torch.Tensor):\n",
    "    out_wav = torch.from_numpy(out_wav)\n",
    "\n",
    "# Normalize audio to prevent clipping\n",
    "out_wav = out_wav / out_wav.abs().max()\n",
    "\n",
    "# Save audio file\n",
    "output_filename = \"bangla_xtts_output.wav\"\n",
    "torchaudio.save(\n",
    "    output_filename,\n",
    "    out_wav,\n",
    "    sample_rate=24000,\n",
    "    encoding=\"PCM_F\",\n",
    "    bits_per_sample=32\n",
    ")\n",
    "\n",
    "print(f\"üéµ Audio saved as: {output_filename}\")\n",
    "print(f\"üìä Sample rate: 24000 Hz\")\n",
    "print(f\"üìä Duration: {out_wav.shape[1] / 24000:.2f} seconds\")\n",
    "print(f\"üìä File size: ~{out_wav.shape[1] * 4 / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Audio Analysis and Visualization (Optional)\n",
    "\n",
    "Analyze the generated audio and create visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create audio waveform visualization\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Waveform plot\n",
    "plt.subplot(2, 1, 1)\n",
    "time_axis = np.linspace(0, out_wav.shape[1] / 24000, out_wav.shape[1])\n",
    "plt.plot(time_axis, out_wav.squeeze().numpy())\n",
    "plt.title('Generated Bangla Speech Waveform (XTTS Fine-tuned)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Spectrogram\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.specgram(out_wav.squeeze().numpy(), Fs=24000, cmap='viridis')\n",
    "plt.title('Spectrogram', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Frequency (Hz)')\n",
    "plt.colorbar(label='Power (dB)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Audio statistics\n",
    "print(\"üìä Audio Statistics:\")\n",
    "print(f\"   ‚Ä¢ Duration: {out_wav.shape[1] / 24000:.2f} seconds\")\n",
    "print(f\"   ‚Ä¢ Sample Rate: 24000 Hz\")\n",
    "print(f\"   ‚Ä¢ Max Amplitude: {out_wav.abs().max():.4f}\")\n",
    "print(f\"   ‚Ä¢ RMS: {torch.sqrt(torch.mean(out_wav**2)):.4f}\")\n",
    "print(f\"   ‚Ä¢ Dynamic Range: {20 * torch.log10(out_wav.abs().max() / (torch.sqrt(torch.mean(out_wav**2)) + 1e-8)):.1f} dB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Model Performance Evaluation\n",
    "\n",
    "Evaluate the fine-tuned model performance and compare with baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multiple Bangla sentences for evaluation\n",
    "test_sentences = [\n",
    "    \"‡¶Ü‡¶Æ‡¶ø ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶≠‡¶æ‡¶∑‡¶æ‡¶Ø‡¶º ‡¶ï‡¶•‡¶æ ‡¶¨‡¶≤‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡¶ø‡•§\",\n",
    "    \"‡¶¢‡¶æ‡¶ï‡¶æ ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡ßá‡¶∞ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ‡•§\",\n",
    "    \"‡¶Ü‡¶ú‡¶ï‡ßá‡¶∞ ‡¶Ü‡¶¨‡¶π‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ ‡¶ñ‡ßÅ‡¶¨‡¶á ‡¶∏‡ßÅ‡¶®‡ßç‡¶¶‡¶∞‡•§\",\n",
    "    \"‡¶∂‡¶ø‡¶ï‡ßç‡¶∑‡¶æ ‡¶ú‡¶æ‡¶§‡¶ø‡¶∞ ‡¶Æ‡ßá‡¶∞‡ßÅ‡¶¶‡¶£‡ßç‡¶°‡•§\",\n",
    "    \"‡¶™‡ßç‡¶∞‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§‡¶ø ‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞ ‡¶ú‡ßÄ‡¶¨‡¶®‡¶ï‡ßá ‡¶∏‡¶π‡¶ú ‡¶ï‡¶∞‡ßá‡¶õ‡ßá‡•§\"\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing model with multiple sentences...\")\n",
    "print(f\"üìù Total test sentences: {len(test_sentences)}\")\n",
    "\n",
    "# Generate audio for each test sentence\n",
    "for i, sentence in enumerate(test_sentences, 1):\n",
    "    print(f\"\\nüéØ Test {i}: {sentence}\")\n",
    "    \n",
    "    # Generate speech\n",
    "    wav_result = XTTS_MODEL.inference(\n",
    "        text=sentence,\n",
    "        language=\"bn\",\n",
    "        gpt_cond_latent=gpt_cond_latent,\n",
    "        speaker_embedding=speaker_embedding,\n",
    "        temperature=0.1,\n",
    "        length_penalty=1.0,\n",
    "        repetition_penalty=10.0,\n",
    "        top_k=20,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    \n",
    "    # Convert to tensor and normalize\n",
    "    test_wav = torch.tensor(wav_result[\"wav\"]).unsqueeze(0)\n",
    "    test_wav = test_wav / test_wav.abs().max()\n",
    "    \n",
    "    # Save individual test file\n",
    "    test_filename = f\"test_{i}_bangla_xtts.wav\"\n",
    "    torchaudio.save(test_filename, test_wav, 24000)\n",
    "    \n",
    "    # Report statistics\n",
    "    duration = test_wav.shape[1] / 24000\n",
    "    print(f\"   ‚úÖ Generated: {duration:.2f}s, saved as {test_filename}\")\n",
    "\n",
    "print(\"\\nüéâ Model evaluation completed!\")\n",
    "print(\"üìÅ All test audio files have been saved for manual evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary and Next Steps\n",
    "\n",
    "Summary of the XTTS fine-tuning pipeline results and recommendations for further improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéâ XTTS Fine-tuning Pipeline Completed Successfully!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nüìã Pipeline Summary:\")\n",
    "print(\"   ‚úÖ Environment setup and repository cloning\")\n",
    "print(\"   ‚úÖ Pretrained checkpoint download\")\n",
    "print(\"   ‚úÖ Bangla vocabulary extension (2000 tokens)\")\n",
    "print(\"   ‚úÖ DVAE training (15 epochs)\")\n",
    "print(\"   ‚úÖ PyTorch compatibility fixes\")\n",
    "print(\"   ‚úÖ GPT training (55 epochs)\")\n",
    "print(\"   ‚úÖ Model loading and inference setup\")\n",
    "print(\"   ‚úÖ Speech synthesis and evaluation\")\n",
    "\n",
    "print(\"\\nüéØ Model Capabilities:\")\n",
    "print(\"   ‚Ä¢ Cross-lingual text-to-speech synthesis\")\n",
    "print(\"   ‚Ä¢ Bangla language support with extended vocabulary\")\n",
    "print(\"   ‚Ä¢ Speaker voice cloning and conditioning\")\n",
    "print(\"   ‚Ä¢ High-quality 24kHz audio generation\")\n",
    "print(\"   ‚Ä¢ Sentence-level synthesis with natural prosody\")\n",
    "\n",
    "print(\"\\nüìä Training Configuration:\")\n",
    "print(f\"   ‚Ä¢ Language: Bangla (bn)\")\n",
    "print(f\"   ‚Ä¢ Vocabulary Size: 2000 tokens\")\n",
    "print(f\"   ‚Ä¢ DVAE Epochs: 15\")\n",
    "print(f\"   ‚Ä¢ GPT Epochs: 55\")\n",
    "print(f\"   ‚Ä¢ Sample Rate: 24000 Hz\")\n",
    "print(f\"   ‚Ä¢ Device: {device}\")\n",
    "\n",
    "print(\"\\nüöÄ Next Steps:\")\n",
    "print(\"   1. Evaluate audio quality manually\")\n",
    "print(\"   2. Test with diverse Bangla text samples\")\n",
    "print(\"   3. Compare with baseline VITS model\")\n",
    "print(\"   4. Optimize inference parameters for better quality\")\n",
    "print(\"   5. Deploy model for production use\")\n",
    "\n",
    "print(\"\\nüí° Recommendations:\")\n",
    "print(\"   ‚Ä¢ Use more training data for better generalization\")\n",
    "print(\"   ‚Ä¢ Experiment with different temperature settings\")\n",
    "print(\"   ‚Ä¢ Fine-tune repetition penalty for specific use cases\")\n",
    "print(\"   ‚Ä¢ Consider multi-speaker training for voice variety\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéµ Fine-tuned XTTS model ready for Bangladeshi Bangla TTS!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
